{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collects url that starts with \"base url\". \n",
    "## slow but efficient for some of the websites where beautifulsoup fails to grab hidden urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107831/2131371108.py:22: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path = '/usr/local/bin/chromedriver', options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting URL 1: https://Mushfiqur-Rahman-Robin.github.io/\n",
      "Visiting URL 2: https://Mushfiqur-Rahman-Robin.github.io/about/\n",
      "Visiting URL 3: https://Mushfiqur-Rahman-Robin.github.io/achievements/\n",
      "Visiting URL 4: https://Mushfiqur-Rahman-Robin.github.io/certifications/\n",
      "Visiting URL 5: https://Mushfiqur-Rahman-Robin.github.io/contact/\n",
      "Visiting URL 6: https://Mushfiqur-Rahman-Robin.github.io/post/\n",
      "Visiting URL 7: https://Mushfiqur-Rahman-Robin.github.io/publications/\n",
      "Visiting URL 8: https://Mushfiqur-Rahman-Robin.github.io/post/project-7/\n",
      "Visiting URL 9: https://Mushfiqur-Rahman-Robin.github.io/post/project-6/\n",
      "Visiting URL 10: https://Mushfiqur-Rahman-Robin.github.io/post/project-5/\n",
      "Visiting URL 11: https://Mushfiqur-Rahman-Robin.github.io/post/project-4/\n",
      "Visiting URL 12: https://Mushfiqur-Rahman-Robin.github.io/post/project-2/\n",
      "Visiting URL 13: https://Mushfiqur-Rahman-Robin.github.io/post/project-1/\n",
      "Visiting URL 14: https://Mushfiqur-Rahman-Robin.github.io/post/page/2/\n",
      "Visiting URL 15: https://Mushfiqur-Rahman-Robin.github.io/post/page/3/\n",
      "Visiting URL 16: https://Mushfiqur-Rahman-Robin.github.io/post/project-3/\n",
      "Number of links extracted: 16\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from collections import deque\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def crawl_website(base_url):\n",
    "    def get_all_urls(url):\n",
    "        visited_urls = set()\n",
    "        visited_urls.add(url)\n",
    "\n",
    "        session = requests.Session()\n",
    "        queue = deque()\n",
    "        queue.append(url)\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run the browser in headless mode (without a visible window)\n",
    "        driver = webdriver.Chrome(executable_path = '/usr/local/bin/chromedriver', options=chrome_options) \n",
    "\n",
    "        count = 0\n",
    "        while queue:\n",
    "            current_url = queue.popleft()\n",
    "            count += 1\n",
    "            print(f'Visiting URL {count}: {current_url}')\n",
    "            try:\n",
    "                reqs = session.get(current_url)\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"ConnectionError: {e}\")\n",
    "                continue\n",
    "\n",
    "            if reqs.status_code == 200:\n",
    "                soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "                expand_elements = soup.find_all(['div', 'span'], attrs={'data-ah-accordion': True, 'data-toggle': True})\n",
    "                for expand_element in expand_elements:\n",
    "                    expand_url = expand_element.get('data-ah-accordion') or expand_element.get('data-toggle')\n",
    "                    if expand_url and expand_url.startswith('/'):\n",
    "                        expand_url = urljoin(base_url, expand_url)\n",
    "                        if expand_url not in visited_urls:\n",
    "                            visited_urls.add(expand_url)\n",
    "                            queue.append(expand_url)\n",
    "\n",
    "                driver.get(current_url)\n",
    "                # WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))  # Wait for the page to load\n",
    "                expanded_content = driver.page_source\n",
    "                expanded_soup = BeautifulSoup(expanded_content, 'html.parser')\n",
    "                for link in expanded_soup.find_all('a'):\n",
    "                    href = link.get('href')\n",
    "                    if href and href.startswith('/'):\n",
    "                        href = urljoin(base_url, href)\n",
    "\n",
    "                    if (\n",
    "                        href\n",
    "                        and href.startswith(base_url)  # Check if the URL starts with the base_url\n",
    "                        and not href in visited_urls\n",
    "                        and not href.endswith(('.pdf', '.jpg', '.png', '.jpeg', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.mp3', '.mp4'))\n",
    "                        and (not link.has_attr('style') or 'display:none' not in link['style'].lower())\n",
    "                        and (not link.has_attr('aria-hidden') or link['aria-hidden'].lower() != 'true')\n",
    "                        and (not link.has_attr('data-wipe-url') or link['data-wipe-url'].lower() != 'true')\n",
    "                    ):\n",
    "                        visited_urls.add(href)\n",
    "                        queue.append(href)\n",
    "\n",
    "        driver.quit()  # Close the browser after crawling\n",
    "\n",
    "        return visited_urls\n",
    "\n",
    "    url = base_url\n",
    "    all_urls = get_all_urls(url)\n",
    "\n",
    "    filtered_urls = [url for url in all_urls if url.startswith(base_url)]  # Filter URLs that start with the base_url\n",
    "    return filtered_urls\n",
    "\n",
    "def save_links_to_file(links, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for link in links:\n",
    "            file.write(link + '\\n')\n",
    "\n",
    "def main():\n",
    "    base_url = 'https://Mushfiqur-Rahman-Robin.github.io/'  # Change this to the website you want to crawl\n",
    "    filtered_urls = crawl_website(base_url)\n",
    "\n",
    "    save_links_to_file(filtered_urls, '../collected_links/selenium-extracted-links.txt')\n",
    "    print(f\"Number of links extracted: {len(filtered_urls)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combokidbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
